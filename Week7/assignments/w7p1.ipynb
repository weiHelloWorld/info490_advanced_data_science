{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "561e14d1756a0b8e8b33a2db17cde032",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This notebook will be collected automatically at **6pm on Monday** from `/home/data_scientist/assignments/Week7` directory on the course JupyterHub server. If you work on this assignment on the course Jupyterhub server, just make sure that you save your work and instructors will pull your notebooks automatically after the deadline. If you work on this assignment locally, the only way to submit assignments is via Jupyterhub, and you have to place the notebook file in the correct directory with the correct file name before the deadline.\n",
    "\n",
    "1. Make sure everything runs as expected. First, restart the kernel (in the menubar, select `Kernel` → `Restart`) and then run all cells (in the menubar, select `Cell` → `Run All`).\n",
    "2. Make sure you fill in any place that says `YOUR CODE HERE`. Do not write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed by the autograder.\n",
    "3. Do not change the file path or the file name of this notebook.\n",
    "4. Make sure that you save your work (in the menubar, select `File` → `Save and CheckPoint`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7.1. Text Analysis.\n",
    "\n",
    "In this problem, we perform basic text analysis tasks,\n",
    "such as accessing data, tokenizing a corpus, and computing token frequencies,\n",
    "on our course syllabus and on the NLTK Reuters corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2ae2f44bae076d61ac4a256a635ee44d",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nose.tools import (\n",
    "    assert_equal,\n",
    "    assert_is_instance,\n",
    "    assert_almost_equal,\n",
    "    assert_true,\n",
    "    assert_false\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first half of the problem, we use our [course syllabus](https://github.com/UI-DataScience/info490-sp16/blob/master/orientation/syllabus.md) as a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "bef40ff651267457a5501204250c604f",
     "grade": false,
     "grade_id": "syllabus_text",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# INFO 490: Advanced Data Science #\n",
      "\n",
      "INFO 490: Advanced Data Science explores advanced concepts in data\n",
      "science by employing a practical approach, including machine learning;\n",
      "probabilistic programming; text, network, and graph analysis; and cloud\n",
      "computing.\n",
      "\n",
      "## Course Goals ##\n",
      "\n",
      "Upon completion of this course, students will be expected to understand\n",
      "advanced data science concepts. Students will learn the practical\n",
      "aspects of applying machine and statistical learning in a variety of\n",
      "contexts, as well as different aspects of cloud computing. Specific\n",
      "concepts that will be covered including supervised and unsupervised\n",
      "learning, dimensional reduction, clustering, probabilistic programming,\n",
      "text mining, graph analysis, network analysis, Hadoop, NoSQL data\n",
      "stores, Spark, and streaming data analysis.\n",
      "\n",
      "## Prerequisites ##\n",
      "\n",
      "As a pre-requisite for this course, you must have mastered the material\n",
      "in *INFO 490: Foundations of Data Science*. Generally, this is\n",
      "demonstrated by having taken this previous course. However, instructor\n",
      "approval can also be granted for those who can demonstrate proficiency\n",
      "in the required topics.\n",
      "\n",
      "Note: At present, we are using the CS department's cluster to run a\n",
      "course JupyterHub server. Each student is running a Dockerized version\n",
      "of the course software stack. This provides many advantages including\n",
      "robustness against crashes, simplicity of deploying software updates,\n",
      "reduced requirements for students (simply a modern web browser, we have\n",
      "used tablets and smart phones), and simplifying assignment submission.\n",
      "You can also run a Docker container locally, as in previous courses, but\n",
      "this approach is not recommended. In addition, if you work locally,\n",
      "since assignments are automatically collected from your cloud-based\n",
      "Docker container, you must ensure that you push local changes to your\n",
      "course cloud Docker container prior to the deadline.\n",
      "\n",
      "As part of the orientation week activities, we have provided a demo\n",
      "[IPython Notebook](notebooks/intro2ip.ipynb) that you should use to get\n",
      "familiar with working in an IPython Notebook on the course JupyterHub\n",
      "server. Each week there will be an `index.ipynb` IPython Notebook that\n",
      "will give you access to the different course notebooks for that week,\n",
      "please use these on the course server to move through the course\n",
      "material.\n",
      "\n",
      "## Texts ##\n",
      "\n",
      "There are no required textbooks for this course. Instead, we will\n",
      "utilize Internet accessible websites, videos, and documentation as\n",
      "supplemental material to the lesson content. We also will include links,\n",
      "as relevant, to readings from books that are freely available to\n",
      "University of Illinois students, staff and faculty via the University's\n",
      "Safari subscription.\n",
      "\n",
      "## Academic Integrity ##\n",
      "\n",
      "Academic honesty is essential to this course and the University. Any\n",
      "instance of academic dishonesty (including but not limited to cheating,\n",
      "plagiarism, falsification of data, and alteration of grades) will be\n",
      "documented in the student's academic file. In addition, **at a minimum**\n",
      "the particular assessment, quiz, or assignment will be given a zero.\n",
      "Serious or repeated offenses may be punished more severely.\n",
      "\n",
      "**Guidelines for collaborative work**: Discussing course material with\n",
      "your classmates is in general a good idea, but each student is expected\n",
      "to do his or her own work. On assignments, you may discuss the problems\n",
      "and concepts behind them, but you are responsible for your own answers.\n",
      "Please do not post code in the forums! Finally, on assessments and\n",
      "quizzes, your answers must of course be your own. For further info, see\n",
      "the [Student Code, Part 4. Academic\n",
      "Integrity](http://www.admin.illinois.edu/policy/code/article_1/a1_1-401.html).\n",
      "\n",
      "## Communication ##\n",
      "\n",
      "The instructional staff will use the [Announcement Forum][af] on the\n",
      "course Moodle to communicate important course information. Do not\n",
      "unsubscribe from this forum or you risk missing important news!\n",
      "\n",
      "The preferred method for student communication in this course is to use\n",
      "the [Q&A Forum][qaf] on the course Moodle. The instructional staff monitors this\n",
      "forum and will respond in less than 24 hours (in general we will respond\n",
      "even faster than this, especially during normal business hours).\n",
      "Furthermore, your fellow students may be able to help even faster. We\n",
      "also encourage you to search this forum prior to making a new post since\n",
      "your question may have already been answered.  You can search a forum on\n",
      "Moodle by using the __Search forums__ tool that is located on the upper\n",
      "right corner of any Moodle forum.\n",
      "\n",
      "If you have a question (that is not answered in this syllabus nor on the\n",
      "online course forums) you can email the instructional staff, however,\n",
      "this should be a last resort. If we feel the question is best answered\n",
      "on the [Q&A Forum][qaf], we reserve the right to post your question and\n",
      "our answer on Moodle. Also, note the information contained in the\n",
      "__Point Reductions__ section of this syllabus.\n",
      "\n",
      "Finally, we have created a [gitter room][gr] for this course. This is a\n",
      "completely public, real-time communication channel that you can also use\n",
      "to ask questions.\n",
      "\n",
      "\n",
      "[qaf]: https://learn.illinois.edu/mod/forum/view.php?id=1325062\n",
      "[af]: https://learn.illinois.edu/mod/forum/view.php?id=1325059\n",
      "[gr]: https://gitter.im/UI-DataScience/info490-sp16\n",
      "\n",
      "## Course Outline ##\n",
      "\n",
      "Note: The following list of topics is tentative. We build the course\n",
      "during the semester for several reasons:\n",
      "\n",
      "1. This is a new course (in a new field)!\n",
      "2. The enrolled students span multiple colleges and even more\n",
      "departments across the University of Illinois.\n",
      "3. Both undergraduate and graduate students are enrolled.\n",
      "\n",
      "As a result, we feel it is imperative to be able to change the planned\n",
      "pace and material to benefit the majority of enrolled students.\n",
      "\n",
      "\n",
      "| **Week** | **Topics**| **Activities**|\n",
      "|----------|-----------|----------------|\n",
      "|*Orientation Week*| Course Overview & Syllabus Review| |\n",
      "|*Week 1*| Introduction to Machine Learning|  |\n",
      "|*Week 2*| General Linear Models |  |\n",
      "|*Week 3*| Introduction to Supervised Machine Learning|  |\n",
      "|*Week 4*| Supervised Machine Learning|  |\n",
      "|*Week 5*| Introduction to Unsupervised Machine Learning| |\n",
      "|*Week 6*| Unsupervised Machine Learning|\n",
      "|*Week 7*| Introduction to Text Mining | |\n",
      "|*Week 8*| Introduction to Social Media Analysis| |\n",
      "|*Week 9*| Advanced Text Mining | |\n",
      "|*Week 10*| Introduction to Network Analysis| |\n",
      "|*Week 11*| Advanced Network Analysis |  |\n",
      "|*Week 12*| Introduction to Cloud Computing| |\n",
      "|*Week 13*| Introduction to NoSQL| |\n",
      "|*Week 14*| Advanced Cloud Computing | | \n",
      "|*Week 15*| Introduction to Deep Learning| |\n",
      "\n",
      "\n",
      "## Weekly Format ##\n",
      "\n",
      "Each week will provide learning objectives and an outline of the\n",
      "activities for that week with a list of all deadlines and corresponding\n",
      "point values for assignments.\n",
      "\n",
      "### Videos ###\n",
      "\n",
      "Each week there will be at least one instructor created video that will\n",
      "offer a broader context for the new week, explain key concepts, and\n",
      "demonstrate important tasks. To view the instructor videos, you will\n",
      "need to login to the Illinois Mediaspace (links are embedded in the\n",
      "relevant weekly overview). You will be given twenty points for viewing\n",
      "each weekly instructor overview video. In case you are wondering,\n",
      "Illinois Mediaspace tracks the viewing of course videos, so we know not\n",
      "only if you load a video, but how much of the video you actually watched.\n",
      "\n",
      "### Readings ###\n",
      "\n",
      "Readings will consist of articles and excerpts from books and Web sites,\n",
      "internet-accessible videos demonstrating a concept, and, in some cases,\n",
      "IPython Notebooks that can be viewed statically on the Github website,\n",
      "or (via the preferred approach) by interacting with them via the course\n",
      "JupyterHub server. You will be required to read and be familiar with the\n",
      "content of these documents. Readings are contextualized as part of the\n",
      "weekly lesson content and are located in the \"Readings\" section of each\n",
      "lesson.\n",
      "\n",
      "### Lessons ###\n",
      "\n",
      "Lessons will expand upon, or clarify key concepts in the reading\n",
      "assignments or supplement or add to the reading. All lessons for a given\n",
      "week must be completed by 6:00 PM Central on Thursday of that week.\n",
      "\n",
      "### Lesson Assessments ###\n",
      "\n",
      "Each week will contain three lesson modules (except for the last week,\n",
      "which will contain only one). A lesson module will will include a Moodle\n",
      "quiz designed to be taken after completing the readings and carefully\n",
      "reviewing the lesson material. Lesson quizzes will allow two attempts,\n",
      "to ensure students have mastered the relevant material before advancing\n",
      "to the next lesson module. The lessons assessments must all be completed\n",
      "by 6:00 PM Central on Thursday of that week.\n",
      "\n",
      "### Assignments ###\n",
      "\n",
      "**Note: This section is still being finalized.**\n",
      "\n",
      "Every week but the first and last will contain an assignment that will\n",
      "involve one or more computational tasks related to the focus for that\n",
      "given week. Your assignment will be automatically collected at the\n",
      "deadline from the course JupyterHub server. These assignments will be\n",
      "automatically graded for your instructor grade, and will also be\n",
      "randomly distributed for peer assessment. You will have **up to five**\n",
      "assignments to grade as part of peer assessment. You will receive thirty\n",
      "points for simply grading your peer's assignments. Your peer assessment\n",
      "score will be worth a maximum of forty points, and we will drop the\n",
      "highest and lowest score and average the three remaining scores.\n",
      "\n",
      "<!--\n",
      "However, you must submit your assignment for peer grading by using the\n",
      "Moodle Assignment tool. (Note we are exploring ways to do this within\n",
      "the course server and may update this process during the course).\n",
      "\n",
      "Each week you will be given instructions on how to complete the\n",
      "assignment, as well as directions on exactly what you must submit to the\n",
      "course Moodle site. Generally this will be in the form of an IPython\n",
      "notebook. \n",
      "-->\n",
      "\n",
      "To receive full credit from instructor grading, your assignment must be\n",
      "submitted prior to the deadline. There will be a 18-hour grace period,\n",
      "in which an assignment can be submitted, albeit with an automatic 50%\n",
      "reduction in the maximum possible score. After this grace period, no\n",
      "assignments will be accepted. The full credit assignment deadline is\n",
      "6:00 PM Central on the Monday following the relevant week. \n",
      "\n",
      "### Peer Review ###\n",
      "\n",
      "Weekly assignments will be reviewed by your course peers, as well as\n",
      "automatic instructor grading. 70 points (out of the maximum 150 points\n",
      "for each assignment) for each weekly assignment submission\n",
      "will derive from peer review, 80 points (out of the maximum 150 points\n",
      "for each assignment) are assigned from automated instructor review. You will\n",
      "receive 30 points each week for simply viewing and grading your peers'\n",
      "assignments. Note that you can (and should) still grade your peers even\n",
      "if you miss an assignment submission. Peer review of an assignment must\n",
      "be completed by 6:00 PM Central on Saturday of the following week (i.e.,\n",
      "you submit your assignment on a Monday and you must peer assess other\n",
      "students assignments by the following Saturday). You will be assigned\n",
      "assignments to grade approximately one hour after the late assignment\n",
      "deadline, thus around 1:00 pm Tuesday afternoon of each week.\n",
      "\n",
      "| **Item** | **Grade** | \n",
      "|----|----| \n",
      "| Instructor Assessment | 80 points |  \n",
      "| Peer Grading | 30 points | \n",
      "|Peer Assessments | 40 points| \n",
      "| **Total** |  **150** points |\n",
      "\n",
      "Note that we will *only* review clearly erroneous peer assessments (this\n",
      "means there needs to be a major problem). Review requests that are\n",
      "deemed insignificant are subject to an *instructor determined point\n",
      "reduction*.\n",
      "\n",
      "### Weekly Quizzes ###\n",
      "\n",
      "In addition to the lesson quizzes, each week will conclude with a weekly\n",
      "quiz. The weekly quiz is designed to test your overall mastery of the\n",
      "content for each given week. Unlike the lesson quizzes, weekly quizzes\n",
      "will be timed and will not allow multiple attempts. The quiz must be\n",
      "completed by 6:00 PM Central on Friday of that week.\n",
      "\n",
      "### Exams ###\n",
      "\n",
      "This course is project-based in its use of assignments that build\n",
      "progressively on content mastery, application, and peer review; there\n",
      "are no exams in this course.\n",
      "\n",
      "## Grading ##\n",
      "\n",
      "### Grading Distribution ###\n",
      "|Assignment | Points| Occurrences| Total Points |\n",
      "|-----------|-----|-----|-----|\n",
      "|Pre-Class Activity: Introduce Yourself| 60|1|60|\n",
      "|Orientation Quiz|70|1|70|\n",
      "|Lesson Assessments|60|14 (Week 15 is only 20 points)|860|\n",
      "|Weekly Quizzes|70|14 (No quiz in Week 15)|980|\n",
      "|Weekly Overview Videos|20|16 (including the Orientation Week Video)|320|\n",
      "|Assignments (Weeks 2-14)|150|13 |1950|\n",
      "|***Total***|||***4240***|\n",
      "\n",
      "Unlike past courses, we do not plan on dropping any weekly scores.\n",
      "\n",
      "### Grading Scale ###\n",
      "\n",
      "Final grades will be graded on a curve, if necessary. The letter grade\n",
      "cutoffs will be set at the traditional 90%, 80%, and 70% limits, and\n",
      "plus/minus will be added if you are within two points of the traditional\n",
      "cutoffs (so 100–98 is an A+ and 90–92 is an A-).\n",
      "\n",
      "|Percentage|Letter Grade|\n",
      "|--------|---------|\n",
      "|98-100|A+|\n",
      "|92-98|A|\n",
      "|90-92|A-|\n",
      "|88-90|B+|\n",
      "|82-88|B|\n",
      "|80-82|B-|\n",
      "|78-80|C+|\n",
      "|72-78|C|\n",
      "|70-72|C-|\n",
      "|68-70|D+|\n",
      "|62-68|D|\n",
      "|60-62|D-|\n",
      "|Below 60|F|\n",
      "\n",
      "### Point Reductions ###\n",
      "\n",
      "This is a large, online course with only one instructor and one teaching\n",
      "assistant. Thus we require that students read the syllabus and search\n",
      "online forums before either emailing us directly or posting a new\n",
      "question in the Moodle forums. Failure to abide by this request may, at\n",
      "the sole discretion of the instructor, result in a loss of five points\n",
      "per **obvious** infraction. Please note that we are not trying to stifle\n",
      "questions (such as why is the FAA server down?). We simply need to\n",
      "minimize the number of such emails/questions we receive.\n",
      "\n",
      "### Extra Credit ###\n",
      "\n",
      "There is a course Wiki hosted on the course github repository. If you\n",
      "have a problem and obtain a solution (either through your own efforts or\n",
      "in partnership with an instructor), consider writing your problem and\n",
      "solution up as a FAQ post in the github wiki. You get extra credit for\n",
      "doing this and also help your classmates!\n",
      "\n",
      "To get credit for your wiki entry you must contact the course assistant,\n",
      "Samantha Thrush. She will review your post and indicate how many points\n",
      "you will receive, and if she would be willing to review an edited post\n",
      "for additional information. You can submit multiple Wiki entries.\n",
      "\n",
      "## Sample Weekly Schedule ##\n",
      "\n",
      "The following table summarizes the typical weekly schedule, where the\n",
      "assignments are collected the Monday following the Friday when quizzes\n",
      "are due.\n",
      "\n",
      "| Task | Days into *Week* | Date/Time |\n",
      "| ---  | ---  | ---     |\n",
      "| Week Opens | 0 | Monday, 12:00 am |\n",
      "| Lessons Completed | 3 | Thursday, 6:00 pm |\n",
      "| Lesson Assessments |  3 | Thursday, 6:00 pm |\n",
      "| Weekly Quiz  |  4 | Friday, 6:00 pm |\n",
      "| Assignment Collected  | 7 |  *The following* Monday, 6:00 pm |\n",
      "| Late Assignments Collected  | 8 |  *The following* Tuesday, 12:00 pm |\n",
      "| Assignments distributed for Peer Assessment  | 8 |  *The following* Tuesday, 1:00 pm |\n",
      "| Peer Assessment Deadline  | 12 |  *The following* Saturday, 6:00 pm |\n",
      "\n",
      "<!--| Assignments uploaded for Peer Assessment  | 8 |  *The following* Tuesday, 12:00 pm | -->\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repo_url = 'https://raw.githubusercontent.com/UI-DataScience/info490-sp16'\n",
    "syllabus_path = 'orientation/syllabus.md'\n",
    "commit_hash = '9a70b4f736963ff9bece424b0b34a393ebd574f9'\n",
    "\n",
    "resp = requests.get('{0}/{1}/{2}'.format(repo_url, commit_hash, syllabus_path))\n",
    "syllabus_text = resp.text\n",
    "\n",
    "assert_is_instance(syllabus_text, str)\n",
    "\n",
    "print(syllabus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "- Tokenize the text string `syllabus_text`.\n",
    "  You should clean up the list of tokens by removing all puntuation tokens\n",
    "  and keeping only tokens with one or more alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "bbd9e93c0a27eb91636e45bdf9cc1cc2",
     "grade": false,
     "grade_id": "tokenize_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    '''\n",
    "    Tokenizes the text string, and returns a list of tokens with\n",
    "    one or more alphanumeric characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: A string.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    words: A list of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pattern = re.compile(r'[^\\w\\s]')\n",
    "    words = [word.lower() for word in nltk.word_tokenize(re.sub(pattern, ' ', text))]\n",
    "    \n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "64d6bd63cc50a60e576ea203b59dca52",
     "grade": false,
     "grade_id": "tokenize_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['info', '490', 'advanced', 'data', 'science'] ... ['following', 'tuesday', '12', '00', 'pm']\n"
     ]
    }
   ],
   "source": [
    "syllabus_words = get_words(syllabus_text)\n",
    "print(syllabus_words[:5], '...', syllabus_words[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8fde293174b3990cf305aef1bf75a8b5",
     "grade": true,
     "grade_id": "tokenize_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(syllabus_words, list)\n",
    "assert_true(all(isinstance(w, str) for w in syllabus_words))\n",
    "assert_equal(len(syllabus_words), 2363)\n",
    "\n",
    "assert_true(all(all(not c.isupper() for c in w) for w in syllabus_words))\n",
    "assert_true(all(any(c.isalnum() for c in w) for w in syllabus_words))\n",
    "\n",
    "assert_equal(syllabus_words[:5], ['info', '490', 'advanced', 'data', 'science'])\n",
    "assert_equal(syllabus_words[-5:], ['following', 'tuesday', '12', '00', 'pm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Diversity\n",
    "\n",
    "- Compute the the number of tokens, number of words, and lexical diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "5ebb1134ccc89ac2e898cca1796ed761",
     "grade": false,
     "grade_id": "count_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def count(words):\n",
    "    '''\n",
    "    Computes the the number of token, number of words, and lexical diversity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: A list of of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 3-tuple of (num_tokens, num_words, lex_div)\n",
    "    num_tokens: An int. The number of tokens in \"words\".\n",
    "    num_words: An int. The number of words in \"words\".\n",
    "    lex_div: A float. The lexical diversity of \"words\".\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    counts = nltk.FreqDist(words)\n",
    "    num_words = len(words)\n",
    "    num_tokens = len(counts)\n",
    "    lex_div = num_words / num_tokens\n",
    "    \n",
    "    \n",
    "    return num_tokens, num_words, lex_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d0a1c5dd1889769c11a5ebcb19376384",
     "grade": false,
     "grade_id": "count_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllabus has 702 tokens and 2363 words for a lexical diversity of 3.366\n"
     ]
    }
   ],
   "source": [
    "num_tokens, num_words, lex_div = count(syllabus_words)\n",
    "print(\"Syllabus has {0} tokens and {1} words for a lexical diversity of {2:4.3f}\"\n",
    "      \"\".format(num_tokens, num_words, lex_div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "fd8857a20c52127e3197495c0752ef25",
     "grade": true,
     "grade_id": "count_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(num_tokens, int)\n",
    "assert_is_instance(num_words, int)\n",
    "assert_is_instance(lex_div, float)\n",
    "\n",
    "assert_equal(num_tokens, 702)\n",
    "assert_equal(num_words, 2363)\n",
    "assert_almost_equal(lex_div, 3.366096866096866)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common occurrences\n",
    "\n",
    "- Compute the most commonly occurring terms and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3a0a252f4c45699cf212986e57da5e85",
     "grade": false,
     "grade_id": "get_most_common_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_most_common(words, ntop):\n",
    "    '''\n",
    "    Computes the most commonly occurring terms and their counts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: A list of of strings.\n",
    "    ntop: An int. The number of most common words that will be returned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of tuple (token, frequency).\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    counts = nltk.FreqDist(words)\n",
    "    most_common = counts.most_common(ntop)\n",
    "    \n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "94cf04986e6ecc93f648aa481d43a51f",
     "grade": false,
     "grade_id": "get_most_common_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term        : Count\n",
      "--------------------\n",
      "the         :  113\n",
      "to          :   58\n",
      "will        :   51\n",
      "and         :   48\n",
      "a           :   47\n",
      "week        :   44\n",
      "of          :   43\n",
      "be          :   39\n",
      "you         :   38\n",
      "course      :   38\n"
     ]
    }
   ],
   "source": [
    "syllabus_most_common = get_most_common(syllabus_words, 10)\n",
    "\n",
    "print('{0:12s}: {1}'.format('Term', 'Count'))\n",
    "print(20*'-')\n",
    "\n",
    "for token, freq in syllabus_most_common:\n",
    "    print('{0:12s}: {1:4d}'.format(token, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c662652a0e8b616310bb9b57ef198b2a",
     "grade": true,
     "grade_id": "get_most_common_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(syllabus_most_common, list)\n",
    "assert_true(all(isinstance(t, tuple) for t in syllabus_most_common))\n",
    "assert_true(all(isinstance(t, str) for t, f in syllabus_most_common))\n",
    "assert_true(all(isinstance(f, int) for t, f in syllabus_most_common))\n",
    "\n",
    "assert_equal(len(get_most_common(syllabus_words, 10)), 10)\n",
    "assert_equal(len(get_most_common(syllabus_words, 20)), 20)\n",
    "\n",
    "assert_equal(\n",
    "    set(syllabus_most_common[:10]),\n",
    "    set([('the', 113), ('to', 58), ('will', 51), ('and', 48), ('a', 47),\n",
    "     ('week', 44), ('of', 43), ('be', 39), ('course', 38), ('you', 38)])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hapax\n",
    "\n",
    "- Write a function that finds all hapexes in a text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "12b4444d65192247304560b057919484",
     "grade": false,
     "grade_id": "find_hapaxes_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_hapaxes(words):\n",
    "    '''\n",
    "    Finds hapexes in \"words\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: A list of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    counts = nltk.FreqDist(words)\n",
    "    hapaxes = counts.hapaxes()\n",
    "    \n",
    "    return hapaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "aa6278b0d73f6e918c8d532b4740819a",
     "grade": false,
     "grade_id": "find_hapaxes_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['why', 'willing', 'wondering', 'working', 'worth', 'would', 'writing', 'www', 'yourself', 'zero']\n"
     ]
    }
   ],
   "source": [
    "syllabus_hapaxes = find_hapaxes(syllabus_words)\n",
    "print(sorted(syllabus_hapaxes)[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e6f33f92c1906d4814d0b0ce66112f32",
     "grade": true,
     "grade_id": "find_hapaxes_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(syllabus_hapaxes, list)\n",
    "assert_true(all(isinstance(w, str) for w in syllabus_hapaxes))\n",
    "assert_equal(len(syllabus_hapaxes), 388)\n",
    "assert_equal(\n",
    "    sorted(syllabus_hapaxes)[-10:],\n",
    "    ['why', 'willing', 'wondering', 'working', 'worth',\n",
    "     'would', 'writing', 'www', 'yourself', 'zero']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK corpus\n",
    "\n",
    "In the second half of the problem, we use the NLTK Reuters corpus. See the [NLTK docs](http://www.nltk.org/book/ch02.html#reuters-corpus) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2cd30c8732682eaf6565fb9504a2ee3a",
     "grade": false,
     "grade_id": "import_reuters",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical diversity in corpus\n",
    "\n",
    "- Compute the the number of token, number of words, and lexical diversity. Use the `words()` function of the reuters object, which includes non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "0521d4010c18311c05037d9e56bc75ec",
     "grade": false,
     "grade_id": "count_corpus_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def count_corpus(corpus):\n",
    "    '''\n",
    "    Computes the the number of token, number of words, and lexical diversity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 3-tuple of (num_tokens, num_words, lex_div)\n",
    "    num_tokens: An int. The number of tokens in \"words\".\n",
    "    num_words: An int. The number of words in \"words\".\n",
    "    lex_div: A float. The lexical diversity of \"words\".\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    words = corpus.words()\n",
    "    counts = nltk.FreqDist(words)\n",
    "    num_words = len(words)\n",
    "    num_tokens = len(counts)\n",
    "    lex_div = num_words / num_tokens\n",
    "    \n",
    "    return num_words, num_tokens, lex_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "13d5ec1770367e7efc73b80ec73df38a",
     "grade": false,
     "grade_id": "count_corpus_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Reuters corpus has 41600 tokens and 1720901 words for a lexical diversity of 41.368\n"
     ]
    }
   ],
   "source": [
    "num_words, num_tokens, lex_div = count_corpus(reuters)\n",
    "print(\"The Reuters corpus has {0} tokens and {1} words for a lexical diversity of {2:4.3f}\"\n",
    "      \"\".format(num_tokens, num_words, lex_div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9b799cbc3bacdfb8bf6762e917153ef0",
     "grade": true,
     "grade_id": "count_corpus_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(num_tokens, int)\n",
    "assert_is_instance(num_words, int)\n",
    "assert_is_instance(lex_div, float)\n",
    "assert_equal(num_tokens, 41600)\n",
    "assert_equal(num_words, 1720901)\n",
    "assert_almost_equal(lex_div, 41.3678125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long words\n",
    "\n",
    "- Search for all words in corpus that are longer than 20 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0a9860d83e53d48c08aecb73ca552ed0",
     "grade": false,
     "grade_id": "get_long_words_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_long_words(corpus, length=20):\n",
    "    '''\n",
    "    Finds all words in \"corpus\" longer than \"length\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus.\n",
    "    length: An int. Default: 22\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    long_words = [word for word in corpus.words() if len(word) > length]\n",
    "    \n",
    "    return long_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7f8b9a9c147d66772deada4a0cafa25a",
     "grade": false,
     "grade_id": "get_long_words_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['discontinuedoperations', 'Beteiligungsgesellschaft', 'Gloeielampenfabrieken', '..........................................', 'Warenhandelsgesellschaft']\n"
     ]
    }
   ],
   "source": [
    "long_words = get_long_words(reuters, length=20)\n",
    "print(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "d19094b789a8656893e56e6a4e5667f7",
     "grade": true,
     "grade_id": "get_long_words_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(long_words, list)\n",
    "assert_true(all(isinstance(w, str) for w in long_words))    \n",
    "assert_equal(len(long_words), 5)\n",
    "assert_equal(\n",
    "    set(long_words),\n",
    "    set([\n",
    "        'discontinuedoperations',\n",
    "        'Warenhandelsgesellschaft',\n",
    "        'Gloeielampenfabrieken',\n",
    "        'Beteiligungsgesellschaft',\n",
    "        '..........................................'\n",
    "        ])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
